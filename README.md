# ğŸ“ Chatbot BKSI

**RAG-based Q&A Chatbot** cho sinh viÃªn TrÆ°á»ng Äáº¡i Há»c BÃ¡ch Khoa - ÄHQG-HCM.

Há»‡ thá»‘ng sá»­ dá»¥ng ká»¹ thuáº­t **Retrieval-Augmented Generation (RAG)** Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ná»™i quy, quy cháº¿ vÃ  hÆ°á»›ng dáº«n cá»§a trÆ°á»ng.

---

## ğŸ“‹ Má»¥c lá»¥c

- [TÃ­nh nÄƒng](#-tÃ­nh-nÄƒng)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#-kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [CÃ´ng nghá»‡ sá»­ dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [Chi tiáº¿t ká»¹ thuáº­t](#-chi-tiáº¿t-ká»¹-thuáº­t)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u hÃ¬nh](#-cáº¥u-hÃ¬nh)

---

## âœ¨ TÃ­nh nÄƒng

| TÃ­nh nÄƒng                    | MÃ´ táº£                                                                 |
| ---------------------------- | --------------------------------------------------------------------- |
| ğŸ“„ **Document Processing**   | Parse PDF, DOCX, PPTX vá»›i Docling + OCR fallback                      |
| ğŸ‡»ğŸ‡³ **Vietnamese Embeddings** | Tá»‘i Æ°u cho tiáº¿ng Viá»‡t vá»›i `dangvantuan/vietnamese-document-embedding` |
| ğŸ—„ï¸ **LanceDB Vector Store**  | Serverless, cross-platform vector database                            |
| âš¡ **Response Caching**      | Cache cÃ¢u tráº£ lá»i Ä‘á»ƒ tÄƒng tá»‘c truy váº¥n láº·p láº¡i                        |
| ğŸ’¬ **Conversation Memory**   | Duy trÃ¬ ngá»¯ cáº£nh há»™i thoáº¡i Ä‘a lÆ°á»£t                                    |
| ğŸ¯ **Reranking**             | Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c vá»›i CrossEncoder                               |
| ğŸ–¥ï¸ **Streamlit UI**          | Giao diá»‡n chat hiá»‡n Ä‘áº¡i, responsive                                   |

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Tá»•ng quan RAG Pipeline

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ Input Layer"]
        USER[("ğŸ‘¤ User")]
        DOCS[("ğŸ“„ Documents<br/>PDF, DOCX, PPTX")]
    end

    subgraph Processing["âš™ï¸ Processing Layer"]
        PARSER["ğŸ“ Document Parser<br/>(Docling + OCR)"]
        CHUNKER["âœ‚ï¸ Text Chunker<br/>(512 tokens, 50 overlap)"]
        EMBEDDER["ğŸ”¢ Embedding Model<br/>(Vietnamese-Document-Embedding)"]
    end

    subgraph Storage["ğŸ’¾ Storage Layer"]
        LANCEDB[("ğŸ—„ï¸ LanceDB<br/>Vector Store")]
        CACHE[("âš¡ SQLite Cache<br/>Response Cache")]
    end

    subgraph Retrieval["ğŸ” Retrieval Layer"]
        RETRIEVER["ğŸ¯ Retriever<br/>(Similarity Search)"]
        RERANKER["ğŸ“Š Reranker<br/>(CrossEncoder)"]
    end

    subgraph Generation["ğŸ¤– Generation Layer"]
        MEMORY["ğŸ’­ Conversation Memory"]
        LLM["ğŸ§  LLM<br/>(OpenRouter API)"]
    end

    subgraph Output["ğŸ“¤ Output Layer"]
        RESPONSE["ğŸ’¬ Response + Sources"]
    end

    DOCS --> PARSER --> CHUNKER --> EMBEDDER --> LANCEDB
    USER --> |"Query"| EMBEDDER
    EMBEDDER --> |"Query Vector"| RETRIEVER
    LANCEDB --> RETRIEVER --> RERANKER
    RERANKER --> |"Top-K Chunks"| LLM
    MEMORY --> LLM
    USER --> MEMORY
    LLM --> RESPONSE
    CACHE -.-> |"Cache Hit"| RESPONSE
    LLM -.-> |"Cache Miss"| CACHE

    style USER fill:#4285F4,color:#fff
    style LANCEDB fill:#34A853,color:#fff
    style LLM fill:#EA4335,color:#fff
    style RESPONSE fill:#FBBC04,color:#000
```

### Luá»“ng xá»­ lÃ½ chi tiáº¿t

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant UI as ğŸ–¥ï¸ Streamlit UI
    participant RC as ğŸ”— RAG Chain
    participant RT as ğŸ¯ Retriever
    participant RR as ğŸ“Š Reranker
    participant VS as ğŸ—„ï¸ Vector Store
    participant EM as ğŸ”¢ Embedding Model
    participant MM as ğŸ’­ Memory
    participant LLM as ğŸ§  LLM (OpenRouter)

    U->>UI: Nháº­p cÃ¢u há»i
    UI->>RC: chat(message, session_id)

    RC->>MM: Láº¥y lá»‹ch sá»­ há»™i thoáº¡i
    MM-->>RC: Conversation history

    RC->>RT: retrieve(query, top_k=5)
    RT->>EM: embed(query)
    EM-->>RT: Query vector
    RT->>VS: similarity_search(vector, k=5)
    VS-->>RT: Top 5 chunks
    RT-->>RC: Retrieved chunks

    RC->>RR: rerank(query, chunks, top_n=3)
    RR-->>RC: Top 3 reranked chunks

    RC->>RC: Format context + history
    RC->>LLM: generate(prompt + context)
    LLM-->>RC: Generated answer

    RC->>MM: LÆ°u turn má»›i
    RC-->>UI: ChatResponse(answer, sources)
    UI-->>U: Hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i + nguá»“n
```

---

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

| ThÃ nh pháº§n          | CÃ´ng nghá»‡                                      | MÃ´ táº£                                |
| ------------------- | ---------------------------------------------- | ------------------------------------ |
| **Package Manager** | [uv](https://github.com/astral-sh/uv)          | Package manager nhanh cho Python     |
| **Vector Database** | [LanceDB](https://lancedb.com/)                | Serverless, embedded vector database |
| **LLM Framework**   | [LangChain](https://langchain.com/)            | Framework cho á»©ng dá»¥ng LLM           |
| **LLM Provider**    | [GroqCloud](https://groq.com)           | API gateway cho nhiá»u LLM models     |
| **Document Parser** | [Docling](https://github.com/DS4SD/docling)    | Parse PDF, DOCX, PPTX thÃ nh Markdown |
| **OCR**             | [EasyOCR](https://github.com/JaidedAI/EasyOCR) | OCR cho PDF scan (há»— trá»£ tiáº¿ng Viá»‡t) |
| **Embeddings**      | [sentence-transformers](https://sbert.net/)    | Vietnamese document embedding model  |
| **Reranking**       | CrossEncoder                                   | ms-marco-MiniLM-L-6-v2 cho reranking |
| **Caching**         | LangChain SQLiteCache                          | Cache response Ä‘á»ƒ tá»‘i Æ°u             |
| **UI**              | [Streamlit](https://streamlit.io/)             | Web UI framework                     |

---

## ğŸ”§ Chi tiáº¿t ká»¹ thuáº­t

### 1. Document Processing

```mermaid
flowchart LR
    subgraph Input
        PDF["ğŸ“„ PDF"]
        DOCX["ğŸ“ DOCX"]
        PPTX["ğŸ“Š PPTX"]
    end

    subgraph Parser["Document Parser"]
        DOCLING["Docling Converter"]
        OCR["EasyOCR Fallback"]
        DETECT{{"Scanned PDF?"}}
    end

    subgraph Output
        MD["ğŸ“ Markdown"]
    end

    PDF --> DETECT
    DETECT -->|"Yes"| OCR --> MD
    DETECT -->|"No"| DOCLING --> MD
    DOCX --> DOCLING
    PPTX --> DOCLING
```

**Chi tiáº¿t:**

- **Docling**: Parse native text tá»« PDF, DOCX, PPTX
- **Scanned PDF Detection**: Kiá»ƒm tra náº¿u PDF lÃ  áº£nh scan (khÃ´ng cÃ³ text layer)
- **EasyOCR Fallback**: OCR vá»›i há»— trá»£ tiáº¿ng Viá»‡t cho scanned PDF
- **Output**: Markdown format Ä‘á»ƒ giá»¯ cáº¥u trÃºc document

### 2. Text Chunking

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Original Document                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chunk 1      â”‚ â”‚   Chunk 2      â”‚ â”‚   Chunk 3      â”‚
â”‚  (512 tokens)  â”‚ â”‚  (512 tokens)  â”‚ â”‚  (512 tokens)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚     50 token overlap â”‚
                â–¼                      â–¼
```

**Tham sá»‘:**

- `chunk_size`: 512 tokens
- `chunk_overlap`: 50 tokens (Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh giá»¯a cÃ¡c chunks)
- **Splitter**: RecursiveCharacterTextSplitter

### 3. Embedding & Vector Store

```mermaid
flowchart LR
    subgraph Embedding
        CHUNK["Text Chunk"] --> MODEL["Vietnamese-Document-Embedding"]
        MODEL --> VECTOR["768-dim Vector"]
    end

    subgraph LanceDB["LanceDB Storage"]
        VECTOR --> TABLE["chunks table"]
        TABLE --> |"Fields"| FIELDS["id, document_id, content,<br/>vector, metadata"]
    end

    subgraph Search
        QUERY["Query"] --> MODEL
        MODEL --> QVEC["Query Vector"]
        QVEC --> SEARCH["Similarity Search"]
        TABLE --> SEARCH
        SEARCH --> RESULTS["Top-K Results"]
    end
```

**Embedding Model:**

- **Model**: `dangvantuan/vietnamese-document-embedding`
- **Dimension**: 768
- **Optimized**: Cho tiáº¿ng Viá»‡t

### 4. Retrieval & Reranking

```mermaid
flowchart TB
    QUERY["ğŸ” Query"] --> EMBED["Embed Query"]
    EMBED --> SEARCH["Vector Search<br/>(Top-K=5)"]
    SEARCH --> CHUNKS["5 Candidate Chunks"]
    CHUNKS --> RERANK["CrossEncoder Rerank"]
    RERANK --> TOP3["Top 3 Most Relevant"]
    TOP3 --> CONTEXT["Build Context"]

    style RERANK fill:#FF6B6B,color:#fff
```

**Two-stage Retrieval:**

1. **Stage 1 - Vector Search**: TÃ¬m top-K chunks báº±ng cosine similarity
2. **Stage 2 - Reranking**: CrossEncoder Ä‘Ã¡nh giÃ¡ láº¡i relevance vÃ  chá»n top-N

### 5. Conversation Memory

```mermaid
flowchart TB
    subgraph Session["Session: abc123"]
        H1["Human: CÃ¡ch Ä‘Äƒng kÃ½ mÃ´n há»c?"]
        A1["AI: Báº¡n cÃ³ thá»ƒ Ä‘Äƒng kÃ½ qua..."]
        H2["Human: CÃ²n thá»i háº¡n thÃ¬ sao?"]
        A2["AI: Thá»i háº¡n Ä‘Äƒng kÃ½ lÃ ..."]
    end

    Session --> CONTEXT["Message History<br/>(max 20 messages)"]
    CONTEXT --> PROMPT["System Prompt +<br/>History + Context + Query"]
    PROMPT --> LLM["LLM"]
```

**Chi tiáº¿t:**

- **In-memory storage**: LÆ°u theo session_id
- **Max messages**: 20 (cÃ³ thá»ƒ cáº¥u hÃ¬nh)
- **Format**: HumanMessage / AIMessage pairs

### 6. LLM Generation

```mermaid
flowchart TB
    subgraph Prompt["Prompt Template"]
        SYS["System: Báº¡n lÃ  trá»£ lÃ½ sinh viÃªn BKSI..."]
        HIST["History: [previous messages]"]
        CTX["Context: [retrieved chunks]"]
        Q["Question: [user query]"]
    end

    Prompt --> LLM["Groq API<br/>(GPT/Claude/etc)"]
    LLM --> ANSWER["Generated Answer"]

    subgraph Response
        ANSWER
        SOURCES["Source Documents"]
    end
```

---

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
Chatbot_BKSI/
â”œâ”€â”€ configs/                    # Cáº¥u hÃ¬nh
â”‚   â”œâ”€â”€ settings.yaml          # CÃ i Ä‘áº·t á»©ng dá»¥ng
â”‚   â””â”€â”€ prompts.yaml           # Prompt templates
â”œâ”€â”€ data/                       # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                   # TÃ i liá»‡u gá»‘c (PDF, DOCX)
â”‚   â””â”€â”€ processed/             # Markdown Ä‘Ã£ xá»­ lÃ½
â”œâ”€â”€ lancedb_data/              # Vector database
â”œâ”€â”€ scripts/                    # CLI scripts
â”‚   â””â”€â”€ cli.py                 # Typer CLI commands
â”œâ”€â”€ src/                        # Source code chÃ­nh
â”‚   â”œâ”€â”€ cache/                 # Response caching
â”‚   â”œâ”€â”€ config.py              # Settings management
â”‚   â”œâ”€â”€ document_processing/   # Parser & Chunker
â”‚   â”‚   â”œâ”€â”€ parser.py         # Docling + OCR parser
â”‚   â”‚   â””â”€â”€ chunker.py        # Text splitter
â”‚   â”œâ”€â”€ embeddings/            # Embedding model
â”‚   â”‚   â””â”€â”€ embedder.py       # sentence-transformers wrapper
â”‚   â”œâ”€â”€ models/                # Pydantic models
â”‚   â”œâ”€â”€ rag/                   # RAG components
â”‚   â”‚   â”œâ”€â”€ chain.py          # Main RAG chain
â”‚   â”‚   â”œâ”€â”€ retriever.py      # Vector retriever
â”‚   â”‚   â”œâ”€â”€ reranker.py       # CrossEncoder reranker
â”‚   â”‚   â””â”€â”€ memory.py         # Conversation memory
â”‚   â”œâ”€â”€ utils/                 # Logging, helpers
â”‚   â””â”€â”€ vectorstore/           # LanceDB wrapper
â”‚       â””â”€â”€ lancedb_store.py
â”œâ”€â”€ ui/                         # User Interface
â”‚   â”œâ”€â”€ streamlit_app.py       # Streamlit application
â”‚   â”œâ”€â”€ style.css              # Custom CSS
â”‚   â””â”€â”€ hero.js                # Hero mode JavaScript
â”œâ”€â”€ .env.example               # Environment template
â”œâ”€â”€ pyproject.toml             # Project config
â””â”€â”€ README.md
```

---

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) package manager

### BÆ°á»›c cÃ i Ä‘áº·t

```bash
# Clone repository
git clone <repository-url>
cd Chatbot_BKSI

# CÃ i Ä‘áº·t dependencies
uv sync

# Copy file cáº¥u hÃ¬nh
cp .env.example .env

# Chá»‰nh sá»­a .env vá»›i API key cá»§a báº¡n
# OPENROUTER_API_KEY=your_key_here
```

---

## ğŸ“– Sá»­ dá»¥ng

### 1. Ingest tÃ i liá»‡u

Äáº·t file PDF, DOCX vÃ o `data/raw/`:

```bash
# Ingest tÃ i liá»‡u má»›i
uv run bksi ingest

# Rebuild toÃ n bá»™ index
uv run bksi ingest --rebuild
```

### 2. Cháº¡y Streamlit UI

```bash
uv run bksi streamlit
```

Truy cáº­p `http://localhost:8501`

### 3. Chat trong Terminal

```bash
uv run bksi chat
```

### 4. XÃ³a cache

```bash
uv run bksi clear-cache
```

---

## âš™ï¸ Cáº¥u hÃ¬nh

### Environment Variables (`.env`)

```bash
# Báº¯t buá»™c
OPENROUTER_API_KEY=your_openrouter_api_key

# TÃ¹y chá»n
LLM_MODEL=openai/gpt-4o-mini
EMBEDDING_MODEL=dangvantuan/vietnamese-document-embedding
EMBEDDING_DEVICE=cpu  # hoáº·c cuda
```

### Settings (`configs/settings.yaml`)

```yaml
llm:
  model: openai/gpt-4o-mini
  temperature: 0.7
  max_tokens: 2048

embeddings:
  model: dangvantuan/vietnamese-document-embedding
  device: cuda # hoáº·c cpu

vectorstore:
  persist_dir: ./lancedb_data
  table_name: chunks

rag:
  top_k: 5 # Sá»‘ chunks retrieve
  chunk_size: 512 # KÃ­ch thÆ°á»›c chunk
  chunk_overlap: 50 # Overlap giá»¯a chunks
  rerank_enabled: true
  rerank_model: ms-marco-MiniLM-L-6-v2
  rerank_top_n: 3 # Sá»‘ chunks sau rerank

cache:
  enabled: true
  directory: ./.cache

memory:
  enabled: true
  max_messages: 20
```
